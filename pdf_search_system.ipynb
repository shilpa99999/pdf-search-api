{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PDF Search System\n",
        "## Comprehensive PDF Document Search with Highlighting and Citations\n",
        "\n",
        "This notebook provides functionality to:\n",
        "- Import and process all PDF files in the workspace\n",
        "- Search for relevant content across all documents\n",
        "- Generate responses with citations, page numbers, and clickable URLs\n",
        "- Highlight relevant content in yellow\n",
        "- Create URLs that redirect to specific pages in PDFs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def install_package(package):\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
        "\n",
        "# Install required packages\n",
        "packages = [\n",
        "    \"PyPDF2\",\n",
        "    \"pymupdf\",\n",
        "    \"sentence-transformers\",\n",
        "    \"faiss-cpu\",\n",
        "    \"numpy\",\n",
        "    \"pandas\",\n",
        "    \"flask\",\n",
        "    \"flask-cors\",\n",
        "    \"nltk\",\n",
        "    \"scikit-learn\"\n",
        "]\n",
        "\n",
        "for package in packages:\n",
        "    try:\n",
        "        __import__(package.replace('-', '_'))\n",
        "        print(f\"{package} already installed\")\n",
        "    except ImportError:\n",
        "        print(f\"Installing {package}...\")\n",
        "        install_package(package)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import PyPDF2\n",
        "import fitz  # pymupdf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import json\n",
        "import re\n",
        "from typing import List, Dict, Tuple\n",
        "import urllib.parse\n",
        "from IPython.display import HTML, display\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Download NLTK data\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "\n",
        "print(\"All packages imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PDFSearchSystem:\n",
        "    def __init__(self, pdf_directory=\".\"):\n",
        "        self.pdf_directory = pdf_directory\n",
        "        self.documents = []\n",
        "        self.embeddings = None\n",
        "        self.index = None\n",
        "        self.model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "        self.tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n",
        "        self.tfidf_matrix = None\n",
        "        \n",
        "    def extract_text_from_pdf(self, pdf_path: str) -> List[Dict]:\n",
        "        \"\"\"Extract text from PDF with page information\"\"\"\n",
        "        documents = []\n",
        "        \n",
        "        try:\n",
        "            # Use PyMuPDF for better text extraction\n",
        "            doc = fitz.open(pdf_path)\n",
        "            \n",
        "            for page_num in range(len(doc)):\n",
        "                page = doc[page_num]\n",
        "                text = page.get_text()\n",
        "                \n",
        "                if text.strip():  # Only add non-empty pages\n",
        "                    # Split into paragraphs\n",
        "                    paragraphs = [p.strip() for p in text.split('\\n\\n') if p.strip()]\n",
        "                    \n",
        "                    for i, paragraph in enumerate(paragraphs):\n",
        "                        if len(paragraph) > 50:  # Only meaningful paragraphs\n",
        "                            documents.append({\n",
        "                                'file_name': os.path.basename(pdf_path),\n",
        "                                'file_path': pdf_path,\n",
        "                                'page_number': page_num + 1,\n",
        "                                'paragraph_index': i,\n",
        "                                'text': paragraph,\n",
        "                                'url': self.generate_pdf_url(pdf_path, page_num + 1)\n",
        "                            })\n",
        "            \n",
        "            doc.close()\n",
        "            print(f\"Extracted {len(documents)} paragraphs from {os.path.basename(pdf_path)}\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {pdf_path}: {str(e)}\")\n",
        "            \n",
        "        return documents\n",
        "    \n",
        "    def generate_pdf_url(self, pdf_path: str, page_number: int) -> str:\n",
        "        \"\"\"Generate a URL that opens PDF at specific page\"\"\"\n",
        "        # Convert to absolute path and encode for URL\n",
        "        abs_path = os.path.abspath(pdf_path)\n",
        "        file_url = f\"file:///{abs_path.replace(os.sep, '/')}\"\n",
        "        # Add page parameter (works with most PDF viewers)\n",
        "        return f\"{file_url}#page={page_number}\"\n",
        "    \n",
        "    def load_all_pdfs(self):\n",
        "        \"\"\"Load all PDF files from directory and subdirectories\"\"\"\n",
        "        pdf_files = []\n",
        "        \n",
        "        # Find all PDF files\n",
        "        for root, dirs, files in os.walk(self.pdf_directory):\n",
        "            for file in files:\n",
        "                if file.lower().endswith('.pdf'):\n",
        "                    pdf_files.append(os.path.join(root, file))\n",
        "        \n",
        "        print(f\"Found {len(pdf_files)} PDF files\")\n",
        "        \n",
        "        # Extract text from all PDFs\n",
        "        all_documents = []\n",
        "        for pdf_file in pdf_files:\n",
        "            docs = self.extract_text_from_pdf(pdf_file)\n",
        "            all_documents.extend(docs)\n",
        "        \n",
        "        self.documents = all_documents\n",
        "        print(f\"Total paragraphs extracted: {len(self.documents)}\")\n",
        "        \n",
        "        return self.documents\n",
        "    \n",
        "    def create_embeddings(self):\n",
        "        \"\"\"Create embeddings for all documents\"\"\"\n",
        "        if not self.documents:\n",
        "            print(\"No documents loaded. Please run load_all_pdfs() first.\")\n",
        "            return\n",
        "        \n",
        "        print(\"Creating embeddings...\")\n",
        "        texts = [doc['text'] for doc in self.documents]\n",
        "        \n",
        "        # Create sentence embeddings\n",
        "        self.embeddings = self.model.encode(texts)\n",
        "        \n",
        "        # Create FAISS index for fast similarity search\n",
        "        dimension = self.embeddings.shape[1]\n",
        "        self.index = faiss.IndexFlatIP(dimension)  # Inner product for cosine similarity\n",
        "        \n",
        "        # Normalize embeddings for cosine similarity\n",
        "        faiss.normalize_L2(self.embeddings)\n",
        "        self.index.add(self.embeddings)\n",
        "        \n",
        "        # Create TF-IDF matrix for keyword-based search\n",
        "        self.tfidf_matrix = self.tfidf_vectorizer.fit_transform(texts)\n",
        "        \n",
        "        print(\"Embeddings created successfully!\")\n",
        "    \n",
        "    def search(self, query: str, top_k: int = 5, hybrid_weight: float = 0.7) -> List[Dict]:\n",
        "        \"\"\"Search for relevant documents using hybrid approach\"\"\"\n",
        "        if self.embeddings is None:\n",
        "            print(\"Embeddings not created. Please run create_embeddings() first.\")\n",
        "            return []\n",
        "        \n",
        "        # Semantic search using embeddings\n",
        "        query_embedding = self.model.encode([query])\n",
        "        faiss.normalize_L2(query_embedding)\n",
        "        \n",
        "        semantic_scores, semantic_indices = self.index.search(query_embedding, top_k * 2)\n",
        "        semantic_scores = semantic_scores[0]\n",
        "        semantic_indices = semantic_indices[0]\n",
        "        \n",
        "        # Keyword search using TF-IDF\n",
        "        query_tfidf = self.tfidf_vectorizer.transform([query])\n",
        "        keyword_scores = cosine_similarity(query_tfidf, self.tfidf_matrix)[0]\n",
        "        \n",
        "        # Combine scores (hybrid approach)\n",
        "        final_scores = {}\n",
        "        \n",
        "        # Add semantic scores\n",
        "        for i, idx in enumerate(semantic_indices):\n",
        "            if idx < len(self.documents):\n",
        "                final_scores[idx] = hybrid_weight * semantic_scores[i]\n",
        "        \n",
        "        # Add keyword scores\n",
        "        for idx, score in enumerate(keyword_scores):\n",
        "            if idx in final_scores:\n",
        "                final_scores[idx] += (1 - hybrid_weight) * score\n",
        "            else:\n",
        "                final_scores[idx] = (1 - hybrid_weight) * score\n",
        "        \n",
        "        # Sort by combined score\n",
        "        sorted_results = sorted(final_scores.items(), key=lambda x: x[1], reverse=True)[:top_k]\n",
        "        \n",
        "        # Prepare results with highlighting\n",
        "        results = []\n",
        "        for idx, score in sorted_results:\n",
        "            doc = self.documents[idx].copy()\n",
        "            doc['relevance_score'] = float(score)\n",
        "            doc['highlighted_text'] = self.highlight_text(doc['text'], query)\n",
        "            results.append(doc)\n",
        "        \n",
        "        return results\n",
        "    \n",
        "    def highlight_text(self, text: str, query: str) -> str:\n",
        "        \"\"\"Highlight query terms in text\"\"\"\n",
        "        query_terms = query.lower().split()\n",
        "        highlighted_text = text\n",
        "        \n",
        "        for term in query_terms:\n",
        "            if len(term) > 2:  # Only highlight meaningful terms\n",
        "                pattern = re.compile(re.escape(term), re.IGNORECASE)\n",
        "                highlighted_text = pattern.sub(\n",
        "                    f'<mark style=\"background-color: yellow; padding: 2px;\">{term}</mark>',\n",
        "                    highlighted_text\n",
        "                )\n",
        "        \n",
        "        return highlighted_text\n",
        "    \n",
        "    def format_search_results(self, results: List[Dict], query: str) -> str:\n",
        "        \"\"\"Format search results as HTML with clickable links\"\"\"\n",
        "        if not results:\n",
        "            return \"<p>No relevant documents found.</p>\"\n",
        "        \n",
        "        html_output = f\"<h3>Search Results for: '{query}'</h3>\\n\"\n",
        "        \n",
        "        for i, result in enumerate(results, 1):\n",
        "            html_output += f\"\"\"\n",
        "            <div style=\"border: 1px solid #ddd; padding: 15px; margin: 10px 0; border-radius: 5px; background-color: #f9f9f9;\">\n",
        "                <h4>Result {i} - Relevance Score: {result['relevance_score']:.3f}</h4>\n",
        "                <p><strong>Document:</strong> {result['file_name']}</p>\n",
        "                <p><strong>Page:</strong> {result['page_number']}</p>\n",
        "                <p><strong>Content:</strong></p>\n",
        "                <div style=\"background-color: white; padding: 10px; border-left: 4px solid #007acc; margin: 10px 0;\">\n",
        "                    {result['highlighted_text']}\n",
        "                </div>\n",
        "                <p><a href=\"{result['url']}\" target=\"_blank\" style=\"color: #007acc; text-decoration: none; font-weight: bold;\">ðŸ“„ Open PDF at Page {result['page_number']}</a></p>\n",
        "            </div>\n",
        "            \"\"\"\n",
        "        \n",
        "        return html_output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the PDF search system\n",
        "pdf_search = PDFSearchSystem(\".\")\n",
        "\n",
        "# Load all PDFs\n",
        "documents = pdf_search.load_all_pdfs()\n",
        "\n",
        "# Display summary of loaded documents\n",
        "if documents:\n",
        "    df = pd.DataFrame(documents)\n",
        "    print(\"\\n=== Document Summary ===\")\n",
        "    print(f\"Total paragraphs: {len(documents)}\")\n",
        "    print(\"\\nDocuments by file:\")\n",
        "    print(df['file_name'].value_counts())\n",
        "    print(\"\\nPages covered:\")\n",
        "    for file_name in df['file_name'].unique():\n",
        "        file_docs = df[df['file_name'] == file_name]\n",
        "        print(f\"  {file_name}: Pages {file_docs['page_number'].min()}-{file_docs['page_number'].max()}\")\n",
        "else:\n",
        "    print(\"No documents were loaded. Please check your PDF files.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create embeddings for semantic search\n",
        "print(\"Creating embeddings for all documents...\")\n",
        "pdf_search.create_embeddings()\n",
        "print(\"\\nReady for search queries!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Interactive search function\n",
        "def search_pdfs(query: str, num_results: int = 5):\n",
        "    \"\"\"Search PDFs and display formatted results\"\"\"\n",
        "    print(f\"Searching for: '{query}'...\\n\")\n",
        "    \n",
        "    results = pdf_search.search(query, top_k=num_results)\n",
        "    \n",
        "    if results:\n",
        "        html_output = pdf_search.format_search_results(results, query)\n",
        "        display(HTML(html_output))\n",
        "        \n",
        "        # Also return structured data for API use\n",
        "        return {\n",
        "            'query': query,\n",
        "            'total_results': len(results),\n",
        "            'results': results\n",
        "        }\n",
        "    else:\n",
        "        print(\"No relevant documents found.\")\n",
        "        return {'query': query, 'total_results': 0, 'results': []}\n",
        "\n",
        "# Example searches\n",
        "print(\"=== Example Search Queries ===\")\n",
        "print(\"You can now search using the search_pdfs() function.\")\n",
        "print(\"Examples:\")\n",
        "print(\"  search_pdfs('data protection rights')\")\n",
        "print(\"  search_pdfs('GDPR compliance')\")\n",
        "print(\"  search_pdfs('consent processing')\")\n",
        "print(\"  search_pdfs('personal data breach')\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example search - GDPR compliance\n",
        "search_result = search_pdfs('GDPR compliance requirements', 3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the search system for use in Flask API\n",
        "import pickle\n",
        "\n",
        "# Save the trained model and data\n",
        "search_data = {\n",
        "    'documents': pdf_search.documents,\n",
        "    'embeddings': pdf_search.embeddings,\n",
        "    'tfidf_vectorizer': pdf_search.tfidf_vectorizer,\n",
        "    'tfidf_matrix': pdf_search.tfidf_matrix\n",
        "}\n",
        "\n",
        "with open('pdf_search_data.pkl', 'wb') as f:\n",
        "    pickle.dump(search_data, f)\n",
        "\n",
        "print(\"Search system data saved successfully!\")\n",
        "print(\"This data will be used by the Flask API.\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
